server:
  address: "0.0.0.0:11434"
  models_path: "/opt/aila/ollama/models"
  gpu: true
  max_runners: 2

models:
  - name: "llama3.1"
    file: "llama3.1.gguf"
    parameters:
      ctx: 8192
      temperature: 0.7
  - name: "mistral"
    file: "mistral-instruct.gguf"
    parameters:
      ctx: 4096
      temperature: 0.5
